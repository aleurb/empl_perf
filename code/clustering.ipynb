{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f778f012",
   "metadata": {},
   "source": [
    "```\n",
    "@title : # Clustering `employee performance` dataset in `Python`\n",
    "@date  : 20241218 ALUR\n",
    "\n",
    "@author: Aleksandras Urbonas, aleksandras . urbonas (.) gmail . com\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64af62",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Intro\n",
    "\n",
    "Clustering a dataset that contains both categorical and continuous variables can be more challenging than clustering datasets with only continuous variables, due to the different nature of the variables.\n",
    "\n",
    "However, with appropriate techniques, it is possible to perform meaningful clustering on this type of data.\n",
    "\n",
    "By comparing properties of the clusters, we will be able to understand the characteristics of each cluster and identify key differences in employee profiles, which could inform management decisions such as resource allocation, promotions, or performance improvement strategies.\n",
    "\n",
    "Visualizations are especially useful for presenting this information to non-technical audiences, while statistical tests provide more formal evidence for your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a165f09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566acf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean data\n",
    "data_0 = pd.read_csv('../data/data_clean.csv', index_col='employee_id')\n",
    "\n",
    "print(data_0.head(2))\n",
    "\n",
    "# data shape:\n",
    "print(data_0.shape, \"\\n\\n *** \\n\\n\")\n",
    "\n",
    "# review data types\n",
    "print(data_0.dtypes, \"\\n\\n *** \\n\\n\")\n",
    "\n",
    "# statistics for numeric values\n",
    "data_0.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in key columns, if any\n",
    "data_1 = data_0.dropna() #subset=['perf_rank', 'is_men', 'is_promo'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates (if any)\n",
    "data_2 = data_1.drop_duplicates()\n",
    "\n",
    "# Status: notify about number of removed duplicates\n",
    "print(f'duplicates removed: {data_1.shape[0] - data_2.shape[0]} records.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop `job_level` components:\n",
    "if 'job_role' in data_2.columns: data_2.drop(columns='job_role', inplace=True)\n",
    "if 'job_rank' in data_2.columns: data_2.drop(columns='job_rank', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f80174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the cleaned data\n",
    "print(data_2.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final assignment: \n",
    "data = data_2\n",
    "data.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7c673",
   "metadata": {},
   "source": [
    "## Key Approaches and Considerations for Clustering Mixed Data (Categorical + Integer Variables):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644ea7f",
   "metadata": {},
   "source": [
    "#### Preprocessing the Data\n",
    "\n",
    "    Proper preprocessing is essential before applying clustering algorithms. This step involves:\n",
    "        - handling missing values,\n",
    "        - scaling continuous variables, and\n",
    "        - encoding categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da36a0",
   "metadata": {},
   "source": [
    "#### Handling Missing Values:\n",
    "\n",
    "        Missing values in categorical variables can be imputed using the mode (most frequent value).\n",
    "        Missing values in continuous variables can be imputed using the mean or median, depending on the distribution of the data.\n",
    "        For clustering, it’s essential that no missing data exists, as most clustering algorithms don’t handle missing values directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7624b0a4",
   "metadata": {},
   "source": [
    "#### Scaling Continuous Variables:\n",
    "\n",
    "    Standardization: Continuous variables should be standardized to have a mean of 0 and a standard deviation of 1. This prevents variables with larger scales (e.g., age or tenure) from dominating the clustering process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c98a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_scaled['age_scaled'] = scaler.fit_transform(data_scaled[['age']])\n",
    "if 'age' in data_scaled.columns: data_scaled.drop(columns='age', inplace=True)\n",
    "data_scaled['tenure_scaled'] = scaler.fit_transform(data_scaled[['tenure']])\n",
    "if 'tenure' in data_scaled.columns: data_scaled.drop(columns='tenure', inplace=True)\n",
    "\n",
    "data_scaled.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504f5cb",
   "metadata": {},
   "source": [
    "#### 1.2 Encoding Categorical Variables:\n",
    "\n",
    "        One-Hot Encoding: This is the most common approach where each category in a categorical variable is converted into a binary feature.\n",
    "            Example: For a column like gender (Male, Female, Non-binary), one-hot encoding would create three new columns: gender_Male, gender_Female, gender_Non-binary.\n",
    "        Label Encoding: This assigns a unique integer to each category.\n",
    "            Example: gender can be encoded as 0 for Male, 1 for Female, 2 for Non-binary.\n",
    "        Frequency or Target Encoding: In some cases, the frequency of the categories or the average target variable (e.g., performance ratings or promotion decisions) can be used to encode categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba712649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "data_dummy = pd.get_dummies(\n",
    "    data_scaled\n",
    "    , columns=['region', 'job_level', 'job_function', 'perf_rank']\n",
    "    , drop_first=False\n",
    ")\n",
    "\n",
    "data_dummy.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbb559",
   "metadata": {},
   "source": [
    "### Columns: categorical or numerical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'gender' and 'job_function' are categorical, 'age' and 'tenure' are continuous\n",
    "continuous_columns = ['age_scaled', 'tenure_scaled']  # Continuous columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba79ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dummy = True\n",
    "\n",
    "if use_dummy == True:\n",
    "    data = data_dummy\n",
    "else:\n",
    "    data = data_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all categorical columns\n",
    "categorical_columns = ['is_promo', 'is_men']  # binary\n",
    "\n",
    "for col in data.columns:\n",
    "    if col not in continuous_columns: \n",
    "        if col not in categorical_columns:\n",
    "            categorical_columns.append(col)\n",
    "\n",
    "print(categorical_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190615d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Choosing Clustering Algorithms for Mixed Data\n",
    "\n",
    "There are several clustering algorithms that can handle both categorical and continuous data effectively:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace431a8",
   "metadata": {},
   "source": [
    "### `K-Prototypes` Clustering:\n",
    "\n",
    "* extension of K-Means, which can handle mixed data types by using different distance measures for categorical and continuous features.\n",
    "* It minimizes a cost function that consists of both categorical and continuous components:\n",
    "    - Continuous variables are handled using Euclidean distance.\n",
    "    - Categorical variables are handled using a dissimilarity measure (e.g., simple matching coefficient).\n",
    "* Requires choosing the number of clusters k in advance, similar to K-Means.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "843ea93e",
   "metadata": {},
   "source": [
    "!pip install kmodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fbf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kpro = data_dummy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ac77d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "# Use K-Prototypes for mixed data clustering\n",
    "kproto = KPrototypes(n_clusters=3, init='Cao', verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446fa93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters = kproto.fit_predict(data_kpro[categorical_columns + continuous_columns], categorical=[0, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2957c06",
   "metadata": {},
   "source": [
    "> Best run was number 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41badf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster label to the dataset\n",
    "data_kpro['cluster'] = clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b7847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_kpro.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b81948",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "\n",
    "To compare the properties of clusters in your original dataset after performing hierarchical clustering, you need to analyze and summarize the data for each cluster.\n",
    "This comparison can provide insights into how the clusters differ based on various attributes (e.g., age, tenure, gender, job function, performance ratings).\n",
    "\n",
    "Here are a few common techniques you can use to compare properties of clusters in your original dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92611c59",
   "metadata": {},
   "source": [
    "1. Descriptive Statistics by Cluster\n",
    "\n",
    "You can calculate basic descriptive statistics (mean, median, standard deviation, etc.) for each cluster to compare the properties of continuous variables such as age, tenure, and performance rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['cluster'] = clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and calculate descriptive statistics\n",
    "cluster_stats = data_2.groupby('cluster').agg({\n",
    "    'age_scaled': ['mean', 'min', 'max'],\n",
    "    'tenure_scaled': ['mean', 'min', 'max'],\n",
    "    'perf_rank': ['mean', 'min', 'max'],\n",
    "    'is_promo': ['mean', 'min', 'max'],\n",
    "    'is_men': ['mean', 'min', 'max'],\n",
    "})\n",
    "\n",
    "print(cluster_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32001cb3",
   "metadata": {},
   "source": [
    "2. Count the Frequency of Categorical Variables in Each Cluster\n",
    "\n",
    "For categorical variables like gender, job_function, you can use a cross-tabulation or group-by operation to see how each category is distributed across clusters.\n",
    "\n",
    "This will give you a count of how many males, females, and non-binary employees belong to each cluster. Similarly, you can look at the distribution of different job_function categories in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b932024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of categorical variables by cluster\n",
    "categorical_comparison = pd.crosstab(data_2['cluster'], data_2['is_men'])\n",
    "print(categorical_comparison)\n",
    "\n",
    "categorical_comparison_function = pd.crosstab(data_2['cluster'], data_2['job_function'])\n",
    "print(categorical_comparison_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5501288",
   "metadata": {},
   "source": [
    "### 3. Compare Cluster Distribution Using Visualization\n",
    "\n",
    "You can use visualizations to better understand how the clusters differ. Common visualization methods include box plots, violin plots, and bar charts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d68a61",
   "metadata": {},
   "source": [
    "a. Box Plot for Continuous Variables\n",
    "\n",
    "Box plots are useful to compare the distributions of continuous variables (like age, tenure, perf_rating) across clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_2\n",
    "data.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8150ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set chart size\n",
    "plt.rcParams[\"figure.figsize\"] = 5, 3\n",
    "sns.set_theme(rc={'figure.figsize':(5, 3)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for 'age' by cluster\n",
    "sns.boxplot(x='cluster', y='age_scaled', data=data)\n",
    "plt.title(\"Age Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for 'tenure' by cluster\n",
    "sns.boxplot(x='cluster', y='tenure_scaled', data=data)\n",
    "plt.title(\"Tenure Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for 'perf_rating' by cluster\n",
    "# sns.boxplot(x='cluster', y='perf_rank', data=data)\n",
    "# plt.title(\"Performance Rating Distribution by Cluster\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9c606",
   "metadata": {},
   "source": [
    "b. Bar Plot for Categorical Variables\n",
    "\n",
    "Bar plots are effective for showing how categorical variables like gender or job_function are distributed across clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ca922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bar plot for gender distribution by cluster\n",
    "sns.countplot(x='cluster', hue='is_men', data=data)\n",
    "plt.title(\"Gender Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for promotion distribution by cluster\n",
    "sns.countplot(x='cluster', hue='is_promo', data=data)\n",
    "plt.title(\"Promotion Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for job_level distribution by cluster\n",
    "sns.countplot(x='cluster', hue='job_level', data=data)\n",
    "plt.title(\"Job Function Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for `perf_rank` distribution by cluster\n",
    "sns.countplot(x='cluster', hue='perf_rank', data=data)\n",
    "plt.title(\"Perf. Rank Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for job_function distribution by cluster\n",
    "sns.countplot(x='cluster', hue='job_function', data=data)\n",
    "plt.title(\"Job Function Distribution by Cluster\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20151803",
   "metadata": {},
   "source": [
    "4. Compare Clusters Using Pivot Tables\n",
    "\n",
    "You can use pivot tables to summarize and compare different properties by cluster. This approach is especially useful for quick comparisons between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for summary statistics of continuous variables\n",
    "pivot_stats = pd.pivot_table(\n",
    "    data_2\n",
    "    , values=['age_scaled', 'tenure_scaled', 'perf_rank']\n",
    "    , index='cluster'\n",
    "    , aggfunc={'age_scaled': ['mean', 'std'], 'tenure_scaled': ['mean', 'std'], 'perf_rank': ['mean', 'std']})\n",
    "\n",
    "print(pivot_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a435d6",
   "metadata": {},
   "source": [
    "5. Statistical Tests Between Clusters\n",
    "\n",
    "If you're interested in testing whether the differences between clusters are statistically significant, you can perform `ANOVA` (for continuous variables) or Chi-squared tests (for categorical variables).\n",
    "\n",
    "a. ANOVA for Continuous Variables\n",
    "\n",
    "ANOVA can help determine if there are significant differences between clusters for continuous variables like age, and tenure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a632fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# ANOVA for 'age'\n",
    "f_stat, p_val = f_oneway(data_2[data_2['cluster'] == 0]['age_scaled'], \n",
    "                          data_2[data_2['cluster'] == 1]['age_scaled'], \n",
    "                          data_2[data_2['cluster'] == 2]['age_scaled'])\n",
    "print(f\"ANOVA for Age - F-statistic: {f_stat}, p-value: {p_val}\")\n",
    "\n",
    "# If p-value < 0.05, the differences between clusters in terms of 'age' are statistically significant\n",
    "if p_val < 0.05:\n",
    "    print(f\"Differences between clusters in terms of 'age' are statistically significant\")\n",
    "else:\n",
    "    print(f\"Differences between clusters in terms of 'age' are not statistically significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8447f",
   "metadata": {},
   "source": [
    "    b. Chi-Squared Test for Categorical Variables\n",
    "\n",
    "    To compare categorical variables, we perform a Chi-squared test to see if the distribution of categories differs significantly across clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Chi-squared test for 'gender' vs. 'cluster'\n",
    "contingency_table = pd.crosstab(data_2['cluster'], data_2['is_men'])\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-squared Test for Gender - p-value: {p_val}\")\n",
    "\n",
    "# If the p-value is less than 0.05, it indicates that the distribution of gender across clusters is significantly different.\n",
    "if p_val < 0.05:\n",
    "    print(f\"Differences between clusters in terms of 'gender' are statistically significant\")\n",
    "else:\n",
    "    print(f\"Differences between clusters in terms of 'gender' are not statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e65972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Chi-squared test for 'gender' vs. 'cluster'\n",
    "contingency_table = pd.crosstab(data_2['cluster'], data_2['is_promo'])\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-squared Test for Promotion - p-value: {p_val}\")\n",
    "\n",
    "# If the p-value is less than 0.05, it indicates that the distribution of gender across clusters is significantly different.\n",
    "if p_val < 0.05:\n",
    "    print(f\"Differences between clusters in terms of 'promotion' are statistically significant\")\n",
    "else:\n",
    "    print(f\"Differences between clusters in terms of 'promotion' are not statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96821e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Chi-squared test for 'region' vs. 'cluster'\n",
    "contingency_table = pd.crosstab(data_2['cluster'], data_2['region'])\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-squared Test for region - p-value: {p_val}\")\n",
    "\n",
    "# If the p-value is less than 0.05, it indicates that the distribution of gender across clusters is significantly different.\n",
    "if p_val < 0.05:\n",
    "    print(f\"Differences between clusters in terms of 'region' are statistically significant\")\n",
    "else:\n",
    "    print(f\"Differences between clusters in terms of 'region' are not statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bc817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Chi-squared test for 'job_function' vs. 'cluster'\n",
    "contingency_table = pd.crosstab(data_2['cluster'], data_2['job_function'])\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-squared Test for `job_function` - p-value: {p_val}\")\n",
    "\n",
    "# If the p-value is less than 0.05, it indicates that the distribution of gender across clusters is significantly different.\n",
    "if p_val < 0.05:\n",
    "    print(f\"Differences between clusters in terms of 'job_function' are statistically significant\")\n",
    "else:\n",
    "    print(f\"Differences between clusters in terms of 'job_function' are not statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a36aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Chi-squared test for 'job_level' vs. 'cluster'\n",
    "contingency_table = pd.crosstab(data_2['cluster'], data_2['job_level'])\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-squared Test for `job_level` - p-value: {p_val}\")\n",
    "\n",
    "# If the p-value is less than 0.05, it indicates that the distribution of gender across clusters is significantly different.\n",
    "if p_val < 0.05:\n",
    "    print(f\"Differences between clusters in terms of 'job_level' are statistically significant\")\n",
    "else:\n",
    "    print(f\"Differences between clusters in terms of 'job_level' are not statistically significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bd1fd",
   "metadata": {},
   "source": [
    "## 6. Cluster Profiling Summary\n",
    "\n",
    "Once you've completed the analysis using the above techniques, you can create a summary of the clusters' characteristics. For example:\n",
    "\n",
    "    Cluster 1: might have a younger average age, higher performance ratings, and a more balanced gender distribution.\n",
    "    Cluster 2: might have employees with higher tenure and predominantly male employees, with lower performance ratings.\n",
    "    Cluster 3: might have a mix of younger and older employees, but more females and a higher rate of promotion readiness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84babaf4",
   "metadata": {},
   "source": [
    "#### K-Means with Mixed Data:\n",
    "\n",
    "    If using K-Means (which requires all numeric data), you must encode categorical variables numerically and standardize continuous variables.\n",
    "\n",
    "    For categorical data, one common approach is One-Hot Encoding or Label Encoding before running the clustering algorithm.\n",
    "    \n",
    "    However, K-Means is typically not ideal for categorical data because it minimizes the sum of squared distances, which is inappropriate for categorical data.\n",
    "\n",
    "#### If K-Means is preferred, apply the following steps:\n",
    "\n",
    "    Encode categorical variables into numeric values (e.g., One-Hot Encoding).\n",
    "    Standardize or normalize continuous variables.\n",
    "    Perform clustering using K-Means as usual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a490d",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering with `Gower` Distance:\n",
    "\n",
    "    Hierarchical Clustering can be used with a distance matrix.\n",
    "    \n",
    "    In this case, the Gower distance can be calculated to handle both continuous and categorical variables.\n",
    "    Gower distance is a metric designed to measure dissimilarity between mixed-type data points.\n",
    "    Hierarchical clustering can be agglomerative (bottom-up) or divisive (top-down).\n",
    "    `Scipy` or `sklearn` allows for hierarchical clustering, but you must compute the custom Gower distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8efd7b1",
   "metadata": {},
   "source": [
    "!pip install gower\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gower = data_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ecc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower\n",
    "\n",
    "# Calculate the Gower distance matrix for dataset\n",
    "distance_matrix = gower.gower_matrix(data_gower)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Perform hierarchical clustering using Ward's method\n",
    "Z = linkage(distance_matrix, method='ward')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c985a13",
   "metadata": {},
   "source": [
    "> `ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix after removing the cwd from sys.path.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0525db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dendrogram to visualize the clustering\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z, labels=data_gower.index)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc67bb8",
   "metadata": {},
   "source": [
    "5. Cutting the Dendrogram to Form Clusters\n",
    "\n",
    "If you want to cut the dendrogram at a certain distance level to form clusters, you can use the fcluster() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819265e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Define the maximum distance for cutting the dendrogram (e.g., cutting at distance < 5)\n",
    "clusters = fcluster(Z, t=100, criterion='distance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db076c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the original dataframe\n",
    "data_gower['cluster'] = clusters\n",
    "data_gower.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f6acd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Self-Organizing Maps (SOMs):\n",
    "\n",
    "    Self-Organizing Maps (SOM) are neural networks that can be used for clustering mixed data types.\n",
    "    SOMs can be trained using both categorical and continuous variables.\n",
    "    They are particularly useful for visualizing high-dimensional data in lower-dimensional representations (2D grid).\n",
    "\n",
    "    `MiniSom` is a Python package to train SOMs for mixed data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "941cb48c",
   "metadata": {},
   "source": [
    "!pip install minisom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_som = data_dummy\n",
    "data_som.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1705ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom\n",
    "\n",
    "# Define the grid size\n",
    "som = MiniSom(10, 10, len(data_som.columns), sigma=0.5, learning_rate=0.5, random_seed=8)\n",
    "som.train(data_som.values, 1000, verbose=True)  # Training on data values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt', \n",
    "                    names=['area', 'perimeter', 'compactness', 'length_kernel', 'width_kernel',\n",
    "                   'asymmetry_coefficient', 'length_kernel_groove', 'target'], usecols=[0, 5], \n",
    "                   sep='\\t+', engine='python')\n",
    "# data normalization\n",
    "data_som = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "data_som = data_som.values\n",
    "\n",
    "# Initialization and training\n",
    "som_shape = (1, 3)\n",
    "som = MiniSom(som_shape[0], som_shape[1], data_som.shape[1], sigma=.5, learning_rate=.5,\n",
    "              neighborhood_function='gaussian', random_seed=10)\n",
    "\n",
    "som.train_batch(data_som, 500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298022ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each neuron represents a cluster\n",
    "winner_coordinates = np.array([som.winner(x) for x in data_som]).T\n",
    "\n",
    "\n",
    "# with np.ravel_multi_index we convert the bidimensional\n",
    "# coordinates to a monodimensional index\n",
    "cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c235a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plotting the clusters using the first 2 dimentions of the data\n",
    "for c in np.unique(cluster_index):\n",
    "    plt.scatter(data_som[cluster_index == c, 0],\n",
    "                data_som[cluster_index == c, 1], label='cluster='+str(c), alpha=.7)\n",
    "\n",
    "# plotting centroids\n",
    "for centroid in som.get_weights():\n",
    "    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x', \n",
    "                s=80, linewidths=3.5, color='k', label='centroid')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23783d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddccb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b5df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e7580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4edc0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained SOM to assign clusters\n",
    "cluster_labels = som._labels\n",
    "data_som['cluster'] = cluster_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8581bb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Evaluation of Clustering\n",
    "\n",
    "Evaluating the quality of clusters is crucial to determine how well your clustering algorithm has performed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73b88f",
   "metadata": {},
   "source": [
    "####  Internal Evaluation Metrics:\n",
    "\n",
    "    Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters.\n",
    "\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    silhouette_avg = silhouette_score(data[['age_scaled', 'tenure_scaled']], clusters)\n",
    "    print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "    Inertia (within-cluster sum of squares): This is the sum of squared distances from each point to its assigned cluster center. Lower values indicate better clustering (for K-Means/K-Prototypes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### External Evaluation Metrics (if ground truth is available):\n",
    "\n",
    "    Adjusted Rand Index (ARI): Measures the similarity between two data clusterings while correcting for chance.\n",
    "\n",
    "    Normalized Mutual Information (NMI): Measures the amount of information shared between two clusterings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb20ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Visualization of Clusters\n",
    "\n",
    "Visualizing clusters is essential to interpret the results and help with reporting.\n",
    "\n",
    "    Pairwise Plots:\n",
    "        Use pairwise plots (scatter plots) to visualize the relationship between different features, coloring by the cluster labels.\n",
    "\n",
    "    import seaborn as sns\n",
    "    sns.pairplot(data, hue='cluster', vars=['age', 'tenure', 'perf_rating'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5340e",
   "metadata": {},
   "source": [
    "#### t-SNE or PCA:\n",
    "\n",
    "    Reduce the data to two dimensions for visualization using t-SNE or PCA (Principal Component Analysis), coloring by cluster labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a870deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA on Categorical columns job_level, job_function, region, job_role\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign work dataset\n",
    "data_pca = data_scaled[['job_level', 'job_function', 'region', 'job_role']]\n",
    "data_pca.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data_pca)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=data_2['cluster'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236e5bd",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Clustering mixed data (categorical + integer) requires careful preprocessing and the choice of clustering algorithm tailored to handle the different data types. The following steps can be taken:\n",
    "\n",
    "    Data Preprocessing: Handle missing values, encode categorical variables, and standardize continuous variables.\n",
    "    Clustering Algorithms: Use specialized algorithms like K-Prototypes, Hierarchical Clustering with Gower distance, or Self-Organizing Maps (SOMs) for mixed data types.\n",
    "    Evaluation: Use internal evaluation metrics (e.g., silhouette score) and external metrics if ground truth is available.\n",
    "    Visualization: Use pairwise plots, t-SNE, or PCA to visualize the clusters.\n",
    "\n",
    "These approaches allow for a deep exploration of mixed data, providing actionable insights from the clustering process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
